{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "This is a file for training the meme_classifier which combines the image feature and text feature.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import classifier_utils as cu\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets, models\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from PIL import ImageFile\n",
    "from torch import optim\n",
    "from collections import Counter\n",
    "import regex as re\n",
    "import time\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score\n",
    "import bcolz\n",
    "import pickle\n",
    "from pytesseract import image_to_string\n",
    "import TwitterDataset as td\n",
    "import MemeModel as mm\n",
    "import train_validation_test_function as tvt\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import help as hp\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for argument 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-dc33d4ce39de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# load test_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mdict_1\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/data/yuhao/web_sci/meme_classifier/test/dic1.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTwitterDataset3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_root\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdict_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomposed_vali\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeme_classifier2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got multiple values for argument 'transform'"
     ]
    }
   ],
   "source": [
    "test_root = '/data/yuhao/web_sci/meme_classifier/test'\n",
    "\n",
    "\n",
    "model_dir = '/data/yuhao/web_sci/meme_classifier/checkpoint/model_text_image.pth'\n",
    "\n",
    "threshold =0.31037354\n",
    "\n",
    "## Loading the pretrained word embedding matrix and backbone for model initiation\n",
    "\n",
    "glove_dir = '/data/yuhao/web_sci/meme_classifier/glove.6B/'\n",
    "vectors = bcolz.open(glove_dir + '6B.50.dat')[:]\n",
    "words = pickle.load(open(glove_dir + '6B.50_words.pkl', 'rb'))\n",
    "word2idx = pickle.load(open(glove_dir + '6B.50_idx.pkl', 'rb'))\n",
    "matrix = torch.FloatTensor(vectors)\n",
    "resnet = models.resnet50(pretrained = True) # load pretrained the resnet50 model\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1]) \n",
    "\n",
    "\n",
    "dict_stat =json.load(open('/data/yuhao/web_sci/meme_classifier/data/name_statistic.json','rb'))\n",
    "## \n",
    "\n",
    "mean_1 = 0.579662\n",
    "mean_2 = 0.5555058\n",
    "mean_3 = 0.5413896\n",
    "std_1 = 0.3494197\n",
    "std_2 = 0.3469673\n",
    "std_3 = 0.35115704\n",
    "\n",
    "composed = transforms.Compose([transforms.Resize((224,224)),transforms.RandomHorizontalFlip(),transforms.ToTensor(),transforms.Normalize(mean=[mean_1, mean_2, mean_3],std=[std_1, std_2, std_3])])\n",
    "\n",
    "composed_vali = transforms.Compose([transforms.Resize((224,224)),transforms.ToTensor(),transforms.Normalize(mean=[mean_1, mean_2, mean_3],std=[std_1, std_2, std_3])])\n",
    "\n",
    "\n",
    "\n",
    "# load test_dataset\n",
    "dict_1 =json.load(open('/data/yuhao/web_sci/meme_classifier/test/dic1.json','rb'))\n",
    "test_dataset = td.TwitterDataset3(test_root,word2idx,dict_1,cu.max_len,transform = composed_vali)\n",
    "model = mm.Meme_classifier2(backbone,matrix,cu.hidden_size2,cu.hidden_size,cu.batch_size)\n",
    "loss = nn.BCELoss()\n",
    "test_loss_p = 1000000\n",
    "index = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2idx\n",
    "# dic_3\n",
    "# dict_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc score is 0.8434054716579581\n",
      "Test ----------------Recall is : 0.7078039927404719\n",
      "Test ----------------Precision is : 0.76171875\n",
      "Test ----------------Accuracy is 0.7891207153502235\n",
      "Test ----------------Test loss is 0.5425280362767363\n",
      "correct!\n"
     ]
    }
   ],
   "source": [
    "gpu = True # use GPU or not\n",
    "model_dir = '/data/yuhao/twitter/MemeClassifier/checkpoint/model.pth'\n",
    "checkpoint = torch.load(model_dir)\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "test_loss2,recall,precision,accuracy,fn,fp,roc,tp = test(model,loss,gpu,test_dataset,threshold)\n",
    "log_3 = 'Test ----------------Recall is : {}'.format(recall)\n",
    "log_4 = 'Test ----------------Precision is : {}'.format(precision)\n",
    "log_5 = 'Test ----------------Accuracy is {}'.format(accuracy)\n",
    "log_6 = 'Test ----------------Test loss is {}'.format(test_loss2)\n",
    "(p,r,t) = roc\n",
    "#out = [(p[i],r[i],t[i]) for i in range(len(r)) if r[i] > 0.7 and p[i] > 0.7  ]\n",
    "#print('feasible cut off is: {}'.format(out))\n",
    "#cu.write_log(log_dir, log_3)\n",
    "#cu.write_log(log_dir, log_4)\n",
    "#cu.write_log(log_dir, log_5)\n",
    "#cu.write_log(log_dir, log_6)\n",
    "print(log_3)\n",
    "print(log_4)\n",
    "print(log_5)\n",
    "print(log_6)\n",
    "fn = [i.split('.')[-2][:-2]+'.png' for i in fn]\n",
    "fp = [i.split('.')[-2][:-2] + '.png' for i in fp]\n",
    "tp = [i.split('.')[-2][:-2] + '.png' for i in tp]\n",
    "#df = pd.read_csv('/data/yuhao/web_sci/meme_classifier/data/vali.csv')\n",
    "#extract_dataframe(df,fn,'/data/yuhao/web_sci/meme_classifier/false_negative.csv')\n",
    "#extract_dataframe(df,fp,'/data/yuhao/web_sci/meme_classifier/false_positive.csv')  \n",
    "#extract_dataframe(df,tp,'/data/yuhao/web_sci/meme_classifier/true_positive.csv')\n",
    "print('correct!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loss, gpu,dataset,threshold):\n",
    "    \"\"\"\n",
    "    This is the function for evaluating the model\n",
    "    Input: model: evaluation model\n",
    "           loader: dataloader\n",
    "           criterion: critertion for loss\n",
    "           gpu: using gpu or not (binary)\n",
    "           dataset: dataset for evaluation\n",
    "    Output: Directory_IWTmeme: \n",
    "            Directory_nonIWTmeme:\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset, batch_size = cu.batch_size, shuffle = True, num_workers = 4)\n",
    "\n",
    "    model.eval()\n",
    "    current_loss = 0\n",
    "    target = []\n",
    "    output = []\n",
    "    preds = []\n",
    "    name = []\n",
    "    for (image_,text_ ,name) in dataloader:\n",
    "\n",
    "        image_,text_ = image_.cuda(),text_.type(torch.LongTensor).cuda()\n",
    "        model = model.cuda() # put data into GPU\n",
    "        output_ = model.forward(image_,text_).type(torch.FloatTensor)\n",
    "        preds.extend(1-output_.detach().cpu().numpy())\n",
    "        name.extend(name)\n",
    "    preds = [True if i > threshold else False for i in preds ]\n",
    "    IWTmeme_name = [i for index,i in enumerate(meme_name) if preds[index]]\n",
    "    nonIWTmeme_name = [i for index,i in enumerate(meme_name) if not preds[index]]\n",
    "    return IWTmeme_name, nonIWTmeme_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which[1,1,1,1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
